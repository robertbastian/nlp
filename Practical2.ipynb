{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from random import shuffle\n",
    "import re\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import lxml.etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile('../1_preprocessing/ted_en-20160408.zip'):\n",
    "    urllib.request.urlretrieve(\"https://wit3.fbk.eu/get.php?path=XML_releases/xml/ted_en-20160408.zip&filename=ted_en-20160408.zip\", filename=\"../1_preprocessing/ted_en-20160408.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('../1_preprocessing/ted_en-20160408.zip', 'r') as z:\n",
    "    doc = lxml.etree.parse(z.open('ted_en-20160408.xml', 'r'))\n",
    "input_keywords = doc.xpath('//keywords/text()')\n",
    "input_contents = doc.xpath('//content/text()')\n",
    "del doc\n",
    "\n",
    "def makeLabel(ks):\n",
    "    return ((\"T\" if \"technology\" in ks else \"o\") + \n",
    "    (\"E\" if \"entertainment\" in ks else \"o\") +\n",
    "    (\"D\" if \"design\" in ks else \"o\"))\n",
    "\n",
    "contents = []\n",
    "keywords = []\n",
    "for (d,k) in zip(input_contents, input_keywords):\n",
    "    # Remove everything in parens\n",
    "    c = re.sub(r'\\([^)]*\\)', '', d)\n",
    "    # Remove line breaks and \"foo: \" prefixes\n",
    "    c = re.sub(r'\\n([^:]{,20}:)?', ' ', c)\n",
    "    # Lowercase, remove special chars\n",
    "    c = re.sub(r'[^a-z0-0\\.]+', ' ', c.lower())\n",
    "    sentences = [s.strip() for s in c.split('.') if len(s.strip()) > 0]\n",
    "    if len(sentences) > 0:\n",
    "        contents.append(sentences)\n",
    "        keywords.append(makeLabel(k))\n",
    "del input_contents\n",
    "del input_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding = \"glove\"\n",
    "vocab = {}\n",
    "vocab_dim = 0\n",
    "\n",
    "if embedding == \"word2vec\":\n",
    "    from gensim.models import Word2Vec\n",
    "    vocab_dim = 100\n",
    "    vocab = Word2Vec([sentence for content in contents for sentence in content], \n",
    "                     size=100, window=5, workers=4)\n",
    "else:\n",
    "    vocab_dim = 50\n",
    "    if not os.path.isfile('glove.6B.zip'):\n",
    "        urllib.request.urlretrieve(\"http://nlp.stanford.edu/data/glove.6B.zip\", filename=\"glove.6B.zip\")\n",
    "    with zipfile.ZipFile('glove.6B.zip', 'r') as z:\n",
    "        fin = z.open('glove.6B.50d.txt', 'r')\n",
    "        for line in fin:\n",
    "            items = line.decode(\"utf-8\").strip().split(' ')\n",
    "            assert len(items) == 51\n",
    "            word = items[0]\n",
    "            vect = np.array([float(i) for i in items[1:]])\n",
    "            vocab[word] = vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "content_vectors = []\n",
    "for content in contents:\n",
    "    sum_, count = np.zeros(vocab_dim), 0\n",
    "    for sentence in content:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                count += 1\n",
    "                sum_ += vocab[word]\n",
    "            except KeyError:\n",
    "                pass\n",
    "    content_vectors.append(sum_ / count)\n",
    "\n",
    "    \n",
    "one_hot = [\"ooo\", \"Too\", \"oEo\", \"ooD\", \"TEo\", \"ToD\", \"oED\", \"TED\"]\n",
    "key_vectors = []\n",
    "for keyword in keywords:\n",
    "    vec = np.zeros(8)\n",
    "    vec[one_hot.index(keyword)] = 1\n",
    "    key_vectors.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.5)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hidden_layer_size = 20\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, vocab_dim])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 8])\n",
    "\n",
    "W = weight_variable([vocab_dim, hidden_layer_size])\n",
    "b = bias_variable([hidden_layer_size])\n",
    "h = tf.tanh(tf.matmul(x, W) + b)\n",
    "\n",
    "V = weight_variable([hidden_layer_size, 8])\n",
    "c = bias_variable([8])\n",
    "u = tf.matmul(h, V) + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(u, y))\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(u, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cross_entropy)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(0, 1585, 50):\n",
    "    sess.run(train_step, feed_dict={x: content_vectors[:1585][i:i+50], \n",
    "                                    y: key_vectors[:1585][i:i+50]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.0107256\n",
      "Test accuracy: 0.044\n"
     ]
    }
   ],
   "source": [
    "print ('Training accuracy: %g' % sess.run(accuracy, feed_dict={\n",
    "            x: content_vectors[:1585], y: key_vectors[:1585]}))\n",
    "print ('Test accuracy: %g' % sess.run(accuracy, feed_dict={\n",
    "            x: content_vectors[-250:], y: key_vectors[-250:]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
